{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b5c956b",
   "metadata": {},
   "source": [
    "# Training Testing on Original Data (All views stacked together)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032b963b",
   "metadata": {},
   "source": [
    "### Loading Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75c2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, BatchNormalization, Conv3D, MaxPooling3D\n",
    "\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9bd1b",
   "metadata": {},
   "source": [
    "### Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f7d246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def callbacks(class_type):\n",
    "    csv_logger = CSVLogger(\"Results/model_history_log_\" + class_type + \".csv\", append=True)\n",
    "    logfilepath = \"Models/setting_model2.csv\"\n",
    "    checkpoint_path = \"Models/cp.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "    reduce_lr_rate=0.5\n",
    "    earlyStopping = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=12, verbose=0, mode='auto')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=reduce_lr_rate, patience=2,\n",
    "                                  cooldown=0, min_lr=0.00001, verbose=0)\n",
    "    callbacks_list = [earlyStopping, reduce_lr, csv_logger]\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x, y, inChannel, z):\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(filters=32, kernel_size=3, activation=\"relu\",input_shape=(x, y, inChannel, z)))\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(Conv3D(filters=64, kernel_size=1, activation=\"relu\",input_shape=(x, y, inChannel, z)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.05))\n",
    "    model.add(MaxPooling3D(pool_size=2,padding='same'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100,activation='relu'))\n",
    "    model.add(Dense(2,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61ef827",
   "metadata": {},
   "source": [
    "## Slices Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e5238",
   "metadata": {},
   "source": [
    "### Train and Valid Axial, Coronal, Saggital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b838b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input_axial = np.load('train/axial.npy')\n",
    "x_input_cor = np.load('train/coronal.npy')\n",
    "x_input_sag = np.load('train/saggital.npy')\n",
    "\n",
    "train_data = np.stack((x_input_axial, x_input_cor, x_input_sag), axis=-1)\n",
    "\n",
    "del x_input_axial\n",
    "del x_input_cor\n",
    "del x_input_sag\n",
    "\n",
    "val_input_axial = np.load('valid/valid_set_axial.npy')\n",
    "val_input_cor = np.load('valid/valid_set_coronal.npy')\n",
    "val_input_sag = np.load('valid/valid_set_sagittal.npy')\n",
    "\n",
    "test_data = np.stack((val_input_axial, val_input_cor, val_input_sag), axis=-1)\n",
    "\n",
    "del val_input_axial\n",
    "del val_input_cor\n",
    "del val_input_sag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee3903",
   "metadata": {},
   "source": [
    "### 1. Abnormal Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c332b",
   "metadata": {},
   "source": [
    "##### Label Loading and converting to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1ec0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_type = 'abnormal'\n",
    "train_labs = pd.read_csv('train-abnormal.csv')\n",
    "valid_labs = pd.read_csv('valid-abnormal.csv')\n",
    "\n",
    "train_1_hot_labs = np.zeros((len(train_labs.Label), 2))\n",
    "val_1_hot_labs = np.zeros((len(valid_labs.Label), 2))\n",
    "\n",
    "ctr = 0\n",
    "for label in train_labs.Label:\n",
    "    if label == 0:\n",
    "        train_1_hot_labs[ctr, 0] = 1\n",
    "    elif label == 1:\n",
    "        train_1_hot_labs[ctr, 1] = 1\n",
    "    ctr+=1\n",
    "\n",
    "ctr = 0\n",
    "for label in valid_labs.Label:\n",
    "    if label == 0:\n",
    "        val_1_hot_labs[ctr, 0] = 1\n",
    "    elif label == 1:\n",
    "        val_1_hot_labs[ctr, 1] = 1\n",
    "    ctr+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79056497",
   "metadata": {},
   "source": [
    "#### Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c194e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_ = 8\n",
    "epochs_ = 50\n",
    "inChannel = train_data.shape[3]\n",
    "x, y, z =  train_data.shape[1], train_data.shape[2], train_data.shape[4]\n",
    "\n",
    "model = create_model(x, y, inChannel, z)\n",
    "print(model.summary())\n",
    "\n",
    "callbacks_list = callbacks(class_type)\n",
    "model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_train = model.fit(train_data, train_1_hot_labs,\n",
    "                    epochs = epochs_, batch_size = batch_size_,\n",
    "                    verbose=1, validation_split=0.15, callbacks=callbacks_list)\n",
    "\n",
    "test_eval=model.evaluate(test_data, val_1_hot_labs, verbose=0)\n",
    "print(\"Test Accuracy: \",test_eval[1])\n",
    "\n",
    "loss = model_train.history['loss']\n",
    "val_loss = model_train.history['val_loss']\n",
    "epochs = range(300)\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b--', label='Training loss')\n",
    "plt.plot(val_loss, 'g--', label='Validation loss')\n",
    "plt.title('Training and validation loss for ' + class_type + ' class')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "acc = model_train.history['accuracy']\n",
    "val_acc = model_train.history['val_accuracy']\n",
    "epochs = range(300)\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b--', label='Training Accuracy')\n",
    "plt.plot(val_acc, 'g--', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy for ' + class_type + ' class')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54d8ff2",
   "metadata": {},
   "source": [
    "### 2. ACL Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a557336",
   "metadata": {},
   "source": [
    "##### Label Loading and converting to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdde94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_type = 'acl'\n",
    "train_labs = pd.read_csv('train-acl.csv')\n",
    "valid_labs = pd.read_csv('valid-acl.csv')\n",
    "\n",
    "train_1_hot_labs = np.zeros((len(train_labs.Label), 2))\n",
    "val_1_hot_labs = np.zeros((len(valid_labs.Label), 2))\n",
    "\n",
    "ctr = 0\n",
    "for label in train_labs.Label:\n",
    "    if label == 0:\n",
    "        train_1_hot_labs[ctr, 0] = 1\n",
    "    elif label == 1:\n",
    "        train_1_hot_labs[ctr, 1] = 1\n",
    "    ctr+=1\n",
    "\n",
    "ctr = 0\n",
    "for label in valid_labs.Label:\n",
    "    if label == 0:\n",
    "        val_1_hot_labs[ctr, 0] = 1\n",
    "    elif label == 1:\n",
    "        val_1_hot_labs[ctr, 1] = 1\n",
    "    ctr+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5235dc",
   "metadata": {},
   "source": [
    "#### Model Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008ade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_ = 8\n",
    "epochs_ = 50\n",
    "inChannel = train_data.shape[3]\n",
    "x, y, z =  train_data.shape[1], train_data.shape[2], train_data.shape[4]\n",
    "\n",
    "model = create_model(x, y, inChannel, z)\n",
    "print(model.summary())\n",
    "\n",
    "callbacks_list = callbacks(class_type)\n",
    "model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_train = model.fit(train_data, train_1_hot_labs,\n",
    "                    epochs = epochs_, batch_size = batch_size_,\n",
    "                    verbose=1, validation_split=0.15, callbacks=callbacks_list)\n",
    "\n",
    "test_eval=model.evaluate(test_data, val_1_hot_labs, verbose=0)\n",
    "print(\"Test Accuracy: \",test_eval[1])\n",
    "\n",
    "loss = model_train.history['loss']\n",
    "val_loss = model_train.history['val_loss']\n",
    "epochs = range(300)\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b--', label='Training loss')\n",
    "plt.plot(val_loss, 'g--', label='Validation loss')\n",
    "plt.title('Training and validation loss for ' + class_type + ' class')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "acc = model_train.history['accuracy']\n",
    "val_acc = model_train.history['val_accuracy']\n",
    "epochs = range(300)\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b--', label='Training Accuracy')\n",
    "plt.plot(val_acc, 'g--', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy for ' + class_type + ' class')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a050530b",
   "metadata": {},
   "source": [
    "### 3. Meniscus Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30e1923",
   "metadata": {},
   "source": [
    "##### Label Loading and converting to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad1961",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_type = 'meniscus'\n",
    "train_labs = pd.read_csv('train-meniscus.csv')\n",
    "valid_labs = pd.read_csv('valid-meniscus.csv')\n",
    "\n",
    "train_1_hot_labs = np.zeros((len(train_labs.Label), 2))\n",
    "val_1_hot_labs = np.zeros((len(valid_labs.Label), 2))\n",
    "\n",
    "ctr = 0\n",
    "for label in train_labs.Label:\n",
    "    if label == 0:\n",
    "        train_1_hot_labs[ctr, 0] = 1\n",
    "    elif label == 1:\n",
    "        train_1_hot_labs[ctr, 1] = 1\n",
    "    ctr+=1\n",
    "\n",
    "ctr = 0\n",
    "for label in valid_labs.Label:\n",
    "    if label == 0:\n",
    "        val_1_hot_labs[ctr, 0] = 1\n",
    "    elif label == 1:\n",
    "        val_1_hot_labs[ctr, 1] = 1\n",
    "    ctr+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4743b4",
   "metadata": {},
   "source": [
    "#### Model Training and Testin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91ef37",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_ = 8\n",
    "epochs_ = 50\n",
    "inChannel = train_data.shape[3]\n",
    "x, y, z =  train_data.shape[1], train_data.shape[2], train_data.shape[4]\n",
    "\n",
    "model = create_model(x, y, inChannel, z)\n",
    "print(model.summary())\n",
    "\n",
    "callbacks_list = callbacks(class_type)\n",
    "model.compile( optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_train = model.fit(train_data, train_1_hot_labs,\n",
    "                    epochs = epochs_, batch_size = batch_size_,\n",
    "                    verbose=1, validation_split=0.15, callbacks=callbacks_list)\n",
    "\n",
    "test_eval=model.evaluate(test_data, val_1_hot_labs, verbose=0)\n",
    "print(\"Test Accuracy: \",test_eval[1])\n",
    "\n",
    "loss = model_train.history['loss']\n",
    "val_loss = model_train.history['val_loss']\n",
    "epochs = range(300)\n",
    "plt.figure()\n",
    "plt.plot(loss, 'b--', label='Training loss')\n",
    "plt.plot(val_loss, 'g--', label='Validation loss')\n",
    "plt.title('Training and validation loss for ' + class_type + ' class')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "acc = model_train.history['accuracy']\n",
    "val_acc = model_train.history['val_accuracy']\n",
    "epochs = range(300)\n",
    "plt.figure()\n",
    "plt.plot(acc, 'b--', label='Training Accuracy')\n",
    "plt.plot(val_acc, 'g--', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy for ' + class_type + ' class')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
